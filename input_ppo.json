{
    "agent": "ppo",
    "params": {
        "agent":{
            "actor":{
                "input_layers": [64,32],
                "output_layers": [32,16],
                "ltsm_size": [128],
                "activation": "gelu"
            },
            "value":{
                "input_layers": [64,32],
                "output_layers": [32,16],
                "ltsm_size": [64],
                "activation": "gelu"
            },
            "epsilon": 0.05,
            "learning_rate": 0.2,
            "epochs": 100,
            "log_interval": 1,
            "num_eval_episodes": 1,
            "eval_interval": 10,
            "collect_num_episodes": 1
        },
        "environment":{
            "early_penalty": 30,
            "holes_penalty": 0.9,
            "height_penalty": 0.7,
            "game_over_penalty": 50.0,
            "line_reward": [100.0,300.0,500.0,800.0],
            "block_placed_reward": 10.0,
            "press_down_reward":0.2 
        }
    },
    "bounds": {
        "agent":{
            "actor":{
                "input_layer_0": [32,200],
                "input_layer_1": [16,100],
                "output_layer_0": [16,100],
                "output_layer_1": [8,50],
                "ltsm_size": [16,256],
                "activation": [0,1]
            },
            "value":{
                "input_layer_0": [32,200],
                "input_layer_1": [16,100],
                "output_layer_0": [16,100],
                "output_layer_1": [8,50],
                "ltsm_size": [16,256],
                "activation": [0,1]
            },
            "learning_rate": [0.01,1],
            "epochs": [25,50],
            "epsilon": [0.01,0.5]
        },
        "environment":{
            "early_penalty": [30.0,40.0],
            "holes_penalty": [0.0,1.0],
            "height_penalty": [0.0,1.0],
            "game_over_penalty": [0.0,200.0],
            "line_single_reward": [100.0,200.0],
            "line_double_reward":[200.0,400.0],
            "line_triple_reward":[300.0,800.0],
            "line_tetris_reward":[400.0,1000.0],
            "block_placed_reward": [1.0,30.0],
            "press_down_reward":[0.0,10.0]
        }
    },
    "to_maximize":"all"
}
